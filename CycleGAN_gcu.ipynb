{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data\n",
    "import random\n",
    "from torchvision import transforms\n",
    "import torchvision.models.vgg as vgg\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from collections import namedtuple\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.nn import functional as F\n",
    "import os , itertools\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model params\n",
    "params = {\n",
    "    'batch_size':12,\n",
    "    'input_size':256,\n",
    "    'resize_scale':256,\n",
    "    'crop_size':256,\n",
    "    'fliplr':True,\n",
    "    'num_epochs':100,\n",
    "    'decay_epoch':50,\n",
    "    'ngf':32,   #number of generator filters\n",
    "    'ndf':64,   #number of discriminator filters\n",
    "    'num_resnet':6, #number of resnet blocks\n",
    "    'lrG':0.0002,    #learning rate for generator\n",
    "    'lrD':0.0002,    #learning rate for discriminator\n",
    "    'beta1':0.5 ,    #beta1 for Adam optimizer\n",
    "    'beta2':0.999 ,  #beta2 for Adam optimizer\n",
    "    'lambdaA':10 ,   #lambdaA for cycle loss\n",
    "    'lambdaB':10,  #lambdaB for cycle loss\n",
    "    'img_form':'jpg'\n",
    "}\n",
    "\n",
    "data_dir = '../../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_np(x):\n",
    "    return x.data.cpu().numpy()\n",
    "def plot_train_result(real_image, gen_image, recon_image, epoch, save=False,  show=True, fig_size=(15, 15)):\n",
    "    fig, axes = plt.subplots(2, 3, figsize=fig_size)\n",
    "    imgs = [to_np(real_image[0]), to_np(gen_image[0]), to_np(recon_image[0]),\n",
    "            to_np(real_image[1]), to_np(gen_image[1]), to_np(recon_image[1])]\n",
    "    for ax, img in zip(axes.flatten(), imgs):\n",
    "        ax.axis('off')\n",
    "        #ax.set_adjustable('box-forced')\n",
    "        # Scale to 0-255\n",
    "        img = img.squeeze()\n",
    "        img = (((img - img.min()) * 255) / (img.max() - img.min())).transpose(1, 2, 0).astype(np.uint8)\n",
    "        ax.imshow(img, cmap=None, aspect='equal')\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "    title = 'Epoch {0}'.format(epoch + 1)\n",
    "    fig.text(0.5, 0.04, title, ha='center')\n",
    "\n",
    "    # save figure\n",
    "    if save:\n",
    "        save_fn = 'Result_epoch_{:d}'.format(epoch+1) + '.png'\n",
    "        plt.savefig(save_fn)\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagePool():\n",
    "    def __init__(self, pool_size):\n",
    "        self.pool_size = pool_size\n",
    "        if self.pool_size > 0:\n",
    "            self.num_imgs = 0\n",
    "            self.images = []\n",
    "\n",
    "    def query(self, images):\n",
    "        if self.pool_size == 0:\n",
    "            return images\n",
    "        return_images = []\n",
    "        for image in images.data:\n",
    "            image = torch.unsqueeze(image, 0)\n",
    "            if self.num_imgs < self.pool_size:\n",
    "                self.num_imgs = self.num_imgs + 1\n",
    "                self.images.append(image)\n",
    "                return_images.append(image)\n",
    "            else:\n",
    "                p = random.uniform(0, 1)\n",
    "                if p > 0.5:\n",
    "                    random_id = random.randint(0, self.pool_size-1)\n",
    "                    tmp = self.images[random_id].clone()\n",
    "                    self.images[random_id] = image\n",
    "                    return_images.append(tmp)\n",
    "                else:\n",
    "                    return_images.append(image)\n",
    "        return_images = Variable(torch.cat(return_images, 0))\n",
    "        return return_images\n",
    "        \n",
    "class DatasetFromFolder(data.Dataset):\n",
    "    def __init__(self, image_dir, subfolder='train', transform=None, resize_scale=None, crop_size=None, fliplr=False):\n",
    "        super(DatasetFromFolder, self).__init__()\n",
    "        self.input_path = os.path.join(image_dir, subfolder)\n",
    "        self.image_filenames = [x for x in sorted(glob(self.input_path+'/*.'+params['img_form']))]\n",
    "        self.image_filenames = [f.replace(self.input_path+'/', '') for f in self.image_filenames]\n",
    "        print(self.image_filenames)\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.resize_scale = resize_scale\n",
    "        self.crop_size = crop_size\n",
    "        self.fliplr = fliplr\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Load Image\n",
    "        img_fn = os.path.join(self.input_path, self.image_filenames[index])\n",
    "        img = Image.open(img_fn).convert('RGB')\n",
    "\n",
    "        # preprocessing\n",
    "        if self.resize_scale:\n",
    "            img = img.resize((self.resize_scale, self.resize_scale), Image.BILINEAR)\n",
    "\n",
    "        if self.crop_size:\n",
    "            x = random.randint(0, self.resize_scale - self.crop_size + 1)\n",
    "            y = random.randint(0, self.resize_scale - self.crop_size + 1)\n",
    "            img = img.crop((x, y, x + self.crop_size, y + self.crop_size))\n",
    "        if self.fliplr:\n",
    "            if random.random() < 0.5:\n",
    "                img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CycleGAN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        conv_block = [  nn.ReflectionPad2d(1),\n",
    "                        nn.Conv2d(in_features, in_features, 3),\n",
    "                        nn.InstanceNorm2d(in_features),\n",
    "                        nn.ReLU(inplace=True),\n",
    "                        nn.ReflectionPad2d(1),\n",
    "                        nn.Conv2d(in_features, in_features, 3),\n",
    "                        nn.InstanceNorm2d(in_features)  ]\n",
    "\n",
    "        self.conv_block = nn.Sequential(*conv_block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.conv_block(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_nc, output_nc, n_residual_blocks=9):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        # Initial convolution block       \n",
    "        model = [   nn.ReflectionPad2d(3),\n",
    "                    nn.Conv2d(input_nc, 64, 7),\n",
    "                    nn.InstanceNorm2d(64),\n",
    "                    nn.ReLU(inplace=True) ]\n",
    "\n",
    "        # Downsampling\n",
    "        in_features = 64\n",
    "        out_features = in_features*2\n",
    "        for _ in range(2):\n",
    "            model += [  nn.Conv2d(in_features, out_features, 3, stride=2, padding=1),\n",
    "                        nn.InstanceNorm2d(out_features),\n",
    "                        nn.ReLU(inplace=True) ]\n",
    "            in_features = out_features\n",
    "            out_features = in_features*2\n",
    "\n",
    "        # Residual blocks\n",
    "        for _ in range(n_residual_blocks):\n",
    "            model += [ResidualBlock(in_features)]\n",
    "\n",
    "        # Upsampling\n",
    "        out_features = in_features//2\n",
    "        for _ in range(2):\n",
    "            model += [  nn.ConvTranspose2d(in_features, out_features, 3, stride=2, padding=1, output_padding=1),\n",
    "                        nn.InstanceNorm2d(out_features),\n",
    "                        nn.ReLU(inplace=True) ]\n",
    "            in_features = out_features\n",
    "            out_features = in_features//2\n",
    "\n",
    "        # Output layer\n",
    "        model += [  nn.ReflectionPad2d(3),\n",
    "                    nn.Conv2d(64, output_nc, 7),\n",
    "                    nn.Tanh() ]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_nc):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        # A bunch of convolutions one after another\n",
    "        model = [   nn.Conv2d(input_nc, 64, 4, stride=2, padding=1),\n",
    "                    nn.LeakyReLU(0.2, inplace=True) ]\n",
    "\n",
    "        model += [  nn.Conv2d(64, 128, 4, stride=2, padding=1),\n",
    "                    nn.InstanceNorm2d(128), \n",
    "                    nn.LeakyReLU(0.2, inplace=True) ]\n",
    "\n",
    "        model += [  nn.Conv2d(128, 256, 4, stride=2, padding=1),\n",
    "                    nn.InstanceNorm2d(256), \n",
    "                    nn.LeakyReLU(0.2, inplace=True) ]\n",
    "\n",
    "        model += [  nn.Conv2d(256, 512, 4, padding=1),\n",
    "                    nn.InstanceNorm2d(512), \n",
    "                    nn.LeakyReLU(0.2, inplace=True) ]\n",
    "\n",
    "        # FCN classification layer\n",
    "        model += [nn.Conv2d(512, 1, 4, padding=1)]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x =  self.model(x)\n",
    "        # Average pooling and flatten\n",
    "        return F.avg_pool2d(x, x.size()[2:]).view(x.size()[0], -1)\n",
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "\n",
    "    elif classname.find(\"BatchNorm\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(size=params['input_size']),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "])\n",
    "#Subfolders - day & night\n",
    "train_data_A = DatasetFromFolder(data_dir, subfolder='he', transform=transform,\n",
    "                                resize_scale=params['resize_scale'], crop_size=params['crop_size'], fliplr=params['fliplr'])\n",
    "train_data_loader_A = torch.utils.data.DataLoader(dataset=train_data_A, batch_size=params['batch_size'], shuffle=True)\n",
    "train_data_B = DatasetFromFolder(data_dir, subfolder='pap', transform=transform,\n",
    "                                resize_scale=params['resize_scale'], crop_size=params['crop_size'], fliplr=params['fliplr'])\n",
    "train_data_loader_B = torch.utils.data.DataLoader(dataset=train_data_B, batch_size=params['batch_size'], shuffle=True)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_real_A_data = train_data_A.__getitem__(11).unsqueeze(0) \n",
    "test_real_B_data = train_data_B.__getitem__(91).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build Model \n",
    "#G_A - Day->Night ; G_B - Night -> Day\n",
    "G_A = Generator(3,3).to(device) \n",
    "G_B = Generator(3,3).to(device)\n",
    "\n",
    "#two Discriminators\n",
    "D_A = Discriminator(3).to(device)\n",
    "D_B = Discriminator(3).to(device)\n",
    "\n",
    "G_A.apply(weights_init_normal)\n",
    "G_B.apply(weights_init_normal)\n",
    "D_A.apply(weights_init_normal)\n",
    "D_B.apply(weights_init_normal)\n",
    "\n",
    "\n",
    "G_optimizer = torch.optim.Adam(itertools.chain(G_A.parameters(), G_B.parameters()), lr=params['lrG'], betas=(params['beta1'], params['beta2']))\n",
    "D_A_optimizer = torch.optim.Adam(D_A.parameters(), lr=params['lrD'], betas=(params['beta1'], params['beta2']))\n",
    "D_B_optimizer = torch.optim.Adam(D_B.parameters(), lr=params['lrD'], betas=(params['beta1'], params['beta2']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_Loss = torch.nn.MSELoss().to(device)\n",
    "L1_Loss = torch.nn.L1Loss().to(device)\n",
    "LossOutput = namedtuple(\"LossOutput\", [\"relu1_2\", \"relu2_2\", \"relu3_3\", \"relu4_3\"])\n",
    "\n",
    "# https://discuss.pytorch.org/t/how-to-extract-features-of-an-image-from-a-trained-model/119/3\n",
    "class LossNetwork(torch.nn.Module):\n",
    "    def __init__(self, vgg_model):\n",
    "        super(LossNetwork, self).__init__()\n",
    "        self.vgg_layers = vgg_model.features\n",
    "        self.layer_name_mapping = {\n",
    "            '3': \"relu1_2\",\n",
    "            '8': \"relu2_2\",\n",
    "            '15': \"relu3_3\",\n",
    "            '22': \"relu4_3\"\n",
    "        }\n",
    "    \n",
    "    def forward(self, x):\n",
    "        output = {}\n",
    "        for name, module in self.vgg_layers._modules.items():\n",
    "            x = module(x)\n",
    "            if name in self.layer_name_mapping:\n",
    "                output[self.layer_name_mapping[name]] = x\n",
    "        return LossOutput(**output)\n",
    "\n",
    "vgg_model = vgg.vgg16(pretrained=True)\n",
    "if torch.cuda.is_available():\n",
    "    vgg_model.to(device)\n",
    "loss_network = LossNetwork(vgg_model)\n",
    "loss_network.eval()\n",
    "del vgg_model\n",
    "\n",
    "def gram_matrix(y):\n",
    "    (b, ch, h, w) = y.size()\n",
    "    features = y.view(b, ch, w * h)\n",
    "    features_t = features.transpose(1, 2)\n",
    "    gram = features.bmm(features_t) / (ch * h * w)\n",
    "    return gram\n",
    "\n",
    "def compStyle(a,b):\n",
    "    #http://pytorch.org/docs/master/notes/autograd.html#volatile\n",
    "    styleB_loss_features = loss_network(Variable(a, volatile=True))\n",
    "    gram_style = [Variable(gram_matrix(y).data, requires_grad=False) for y in styleB_loss_features]\n",
    "        \n",
    "    features_y = loss_network(b)\n",
    "        \n",
    "    style_loss = 0    \n",
    "    for m in range(len(features_y)):\n",
    "        gram_s = gram_style[m]\n",
    "        gram_y = gram_matrix(features_y[m])\n",
    "        style_loss += 1e4 * MSE_Loss(gram_y, gram_s.expand_as(gram_y))\n",
    "    return style_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_A_avg_losses = []\n",
    "D_B_avg_losses = []\n",
    "G_A_avg_losses = []\n",
    "G_B_avg_losses = []\n",
    "cycle_A_avg_losses = []\n",
    "cycle_B_avg_losses = []\n",
    "STYLE_WEIGHT = 1e4\n",
    "\n",
    "num_pool = 50\n",
    "fake_A_pool = ImagePool(num_pool)\n",
    "fake_B_pool = ImagePool(num_pool)\n",
    "import tqdm\n",
    "\n",
    "step = 0\n",
    "for epoch in range(params['num_epochs']):\n",
    "    D_A_losses = []\n",
    "    D_B_losses = []\n",
    "    G_A_losses = []\n",
    "    G_B_losses = []\n",
    "    cycle_A_losses = []\n",
    "    cycle_B_losses = []\n",
    "    \n",
    "    # Learing rate decay \n",
    "    if(epoch + 1) > params['decay_epoch']:\n",
    "        D_A_optimizer.param_groups[0]['lr'] -= params['lrD'] / (params['num_epochs'] - params['decay_epoch'])\n",
    "        D_B_optimizer.param_groups[0]['lr'] -= params['lrD'] / (params['num_epochs'] - params['decay_epoch'])\n",
    "        G_optimizer.param_groups[0]['lr'] -= params['lrG'] / (params['num_epochs'] - params['decay_epoch'])\n",
    "        \n",
    "\n",
    "    iteratorA=tqdm.tqdm_notebook(train_data_loader_A)\n",
    "    iteratorB=tqdm.tqdm_notebook(train_data_loader_B)\n",
    "    for i, (real_A, real_B) in enumerate(zip(iteratorA, iteratorB)):\n",
    "        \n",
    "        # input image data\n",
    "        real_A = real_A.to(device)\n",
    "        real_B = real_B.to(device)\n",
    "        \n",
    "        # -------------------------- train generator G --------------------------\n",
    "        # A --> B\n",
    "        fake_B = G_A(real_A)\n",
    "        a_idt = G_A(real_A)\n",
    "        \n",
    "        D_B_fake_decision = D_B(fake_B)\n",
    "        G_A_loss = MSE_Loss(D_B_fake_decision, Variable(torch.ones(D_B_fake_decision.size()).to(device)))\n",
    "        \n",
    "        # forward cycle loss\n",
    "        recon_A = G_B(fake_B)\n",
    "        cycle_A_loss = L1_Loss(recon_A, real_A) * params['lambdaA']\n",
    "        \n",
    "        #idtA_loss = L1_Loss(a_idt,real_A) * 10*0.5 \n",
    "        \n",
    "        styleA_loss = compStyle(real_A,a_idt) \n",
    "        \n",
    "    \n",
    "        #G_B_loss = G_B_loss + (style_loss)/2\n",
    "       \n",
    "        #ends here\n",
    "        \n",
    "        # B --> A\n",
    "        fake_A = G_B(real_B)\n",
    "        b_idt = G_B(real_B)\n",
    "        \n",
    "        D_A_fake_decision = D_A(fake_A)\n",
    "        G_B_loss = MSE_Loss(D_A_fake_decision, Variable(torch.ones(D_A_fake_decision.size()).to(device)))\n",
    "        \n",
    "        # backward cycle loss\n",
    "        recon_B = G_A(fake_A)\n",
    "        cycle_B_loss = L1_Loss(recon_B, real_B) * params['lambdaB']\n",
    "        \n",
    "        #idtB_loss = L1_Loss(b_idt,real_B) * 10*0.5 \n",
    "    \n",
    "        styleB_loss = compStyle(real_B,b_idt) \n",
    "\n",
    "        style_loss = (styleB_loss + styleA_loss)\n",
    "        \n",
    "        # Back propagation\n",
    "        G_loss = G_A_loss + G_B_loss + cycle_A_loss + cycle_B_loss \n",
    "        \n",
    "        G_loss = G_loss+style_loss * 2.5\n",
    "        \n",
    "        \n",
    "        G_optimizer.zero_grad()\n",
    "        G_loss.backward()\n",
    "        G_optimizer.step()\n",
    "    \n",
    "        \n",
    "        # -------------------------- train discriminator D_A --------------------------\n",
    "        D_A_real_decision = D_A(real_A)\n",
    "        D_A_real_loss = MSE_Loss(D_A_real_decision, Variable(torch.ones(D_A_real_decision.size()).to(device)))\n",
    "        \n",
    "        fake_A = fake_A_pool.query(fake_A)\n",
    "        \n",
    "        D_A_fake_decision = D_A(fake_A)\n",
    "        D_A_fake_loss = MSE_Loss(D_A_fake_decision, Variable(torch.zeros(D_A_fake_decision.size()).to(device)))\n",
    "        \n",
    "       # D_A_recon_decision = D_A(recon_A)\n",
    "        #D_A_recon_loss = MSE_Loss(D_A_recon_decision, Variable(torch.zeros(D_A_recon_decision.size()).to(device)))\n",
    "        \n",
    "        # Back propagation\n",
    "        D_A_loss = (D_A_real_loss + D_A_fake_loss ) * 0.5\n",
    "        D_A_optimizer.zero_grad()\n",
    "        D_A_loss.backward()\n",
    "        D_A_optimizer.step()\n",
    "        \n",
    "        \n",
    "        # -------------------------- train discriminator D_B --------------------------\n",
    "        D_B_real_decision = D_B(real_B)\n",
    "        D_B_real_loss = MSE_Loss(D_B_real_decision, Variable(torch.ones(D_B_fake_decision.size()).to(device)))\n",
    "        \n",
    "        fake_B = fake_B_pool.query(fake_B)\n",
    "        \n",
    "        D_B_fake_decision = D_B(fake_B)\n",
    "        D_B_fake_loss = MSE_Loss(D_B_fake_decision, Variable(torch.zeros(D_B_fake_decision.size()).to(device)))\n",
    "        \n",
    "        #D_B_recon_decision = D_B(recon_B)\n",
    "        #D_B_recon_loss = MSE_Loss(D_B_recon_decision, Variable(torch.zeros(D_B_recon_decision.size()).to(device)))\n",
    "        \n",
    "        # Back propagation\n",
    "        D_B_loss = (D_B_real_loss + D_B_fake_loss ) * 0.5\n",
    "        D_B_optimizer.zero_grad()\n",
    "        D_B_loss.backward()\n",
    "        D_B_optimizer.step()\n",
    "        \n",
    "        # ------------------------ Print -----------------------------\n",
    "        # loss values\n",
    "        D_A_losses.append(D_A_loss.item())\n",
    "        D_B_losses.append(D_B_loss.item())\n",
    "        G_A_losses.append(G_A_loss.item())\n",
    "        G_B_losses.append(G_B_loss.item())\n",
    "        cycle_A_losses.append(cycle_A_loss.item())\n",
    "        cycle_B_losses.append(cycle_B_loss.item())\n",
    "\n",
    "        iteratorA.set_description(f\"epoch: {epoch+1}/{params['num_epochs']} Step: {i+1} D_A loss :{D_A_loss.item():.4f} G_A loss :{G_A_loss.item():.4f} cycle_A_losses :{cycle_A_loss.item():.4f}\")    \n",
    "        iteratorB.set_description(f\"epoch: {epoch+1}/{params['num_epochs']} Step: {i+1} D_B loss :{D_B_loss.item():.4f} G_B loss :{G_B_loss.item():.4f} cycle_B_losses :{cycle_B_loss.item():.4f}\")\n",
    "        step += 1\n",
    "        \n",
    "    D_A_avg_loss = torch.mean(torch.FloatTensor(D_A_losses))\n",
    "    D_B_avg_loss = torch.mean(torch.FloatTensor(D_B_losses))\n",
    "    G_A_avg_loss = torch.mean(torch.FloatTensor(G_A_losses))\n",
    "    G_B_avg_loss = torch.mean(torch.FloatTensor(G_B_losses))\n",
    "    cycle_A_avg_loss = torch.mean(torch.FloatTensor(cycle_A_losses))\n",
    "    cycle_B_avg_loss = torch.mean(torch.FloatTensor(cycle_B_losses))\n",
    "    \n",
    "    # avg loss values for plot\n",
    "    D_A_avg_losses.append(D_A_avg_loss.item())\n",
    "    D_B_avg_losses.append(D_B_avg_loss.item())\n",
    "    G_A_avg_losses.append(G_A_avg_loss.item())\n",
    "    G_B_avg_losses.append(G_B_avg_loss.item())\n",
    "    cycle_A_avg_losses.append(cycle_A_avg_loss.item())\n",
    "    cycle_B_avg_losses.append(cycle_B_avg_loss.item())\n",
    "    torch.save(G_A.state_dict(), \"../../model/pap_he/G_A\"+'_'+str(epoch)+\".pth\")\n",
    "    torch.save(G_B.state_dict(), \"../../model/pap_he/G_B\"+'_'+str(epoch)+\".pth\")\n",
    "    torch.save(D_A.state_dict(), \"../../model/pap_he/D_A\"+'_'+str(epoch)+\".pth\")\n",
    "    torch.save(D_B.state_dict(), \"../../model/pap_he/D_B\"+'_'+str(epoch)+\".pth\")\n",
    "    test_real_A = test_real_A_data.to(device)\n",
    "    test_fake_B = G_A(test_real_A)\n",
    "    test_recon_A = G_B(test_fake_B)\n",
    "\n",
    "    test_real_B = test_real_B_data.to(device)\n",
    "    test_fake_A = G_B(test_real_B)\n",
    "    test_recon_B = G_A(test_fake_A)\n",
    "\n",
    "    plot_train_result([test_real_A, test_real_B], [test_fake_B, test_fake_A], [test_recon_A, test_recon_B],\n",
    "                            epoch, save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cProfile import label\n",
    "\n",
    "\n",
    "epoch=np.linspace(0,99,num=100,dtype=np.int16)\n",
    "fig=plt.figure(figsize=(16,15))\n",
    "plt.subplot(3, 1, 1)                # nrows=2, ncols=1, index=1\n",
    "plt.plot(epoch, G_A_avg_losses,'r',label='G_A_loss')\n",
    "plt.plot(epoch, G_B_avg_losses,'b',label='G_B_loss')\n",
    "plt.ylabel('Genarator_loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.subplot(3, 1, 2)                # nrows=2, ncols=1, index=1\n",
    "plt.plot(epoch, D_A_avg_losses,'r',label='D_A_loss')\n",
    "plt.plot(epoch, D_B_avg_losses,'b',label='D_B_loss')\n",
    "plt.ylabel('discriminator_loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.subplot(3, 1, 3)                # nrows=2, ncols=1, index=1\n",
    "plt.plot(epoch, cycle_A_avg_losses,'r',label='cycle_A_loss')\n",
    "plt.plot(epoch, cycle_B_avg_losses,'b',label='cycle_B_loss')\n",
    "plt.ylabel('cycle_loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_A = train_data_A\n",
    "test_data_loader_A = torch.utils.data.DataLoader(dataset=test_data_A, batch_size=params['batch_size'], shuffle=False)\n",
    "test_data_B = train_data_B\n",
    "test_data_loader_B = torch.utils.data.DataLoader(dataset=test_data_B, batch_size=params['batch_size'], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i, (real_A, real_B) in enumerate(zip(train_data_loader_A, train_data_loader_B)):\n",
    "    if(count<50):\n",
    "        real_A = real_A.to(device)\n",
    "        real_B = real_B.to(device)\n",
    "\n",
    "        fake_B = G_A(real_A)\n",
    "        test_recon_A = G_B(fake_B)\n",
    "\n",
    "        fake_A = G_B(real_B)\n",
    "        test_recon_B = G_A(fake_A)\n",
    "\n",
    "        plot_train_result([real_A, real_B], [fake_B, fake_A], [test_recon_A, test_recon_B],count, save = True)\n",
    "        count = count+1\n",
    "    else:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LeeYS_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
